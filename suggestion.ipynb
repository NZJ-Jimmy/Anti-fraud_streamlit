{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fraud_msg_cls\n",
    "import streamlit as st\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "\n",
    "openai.api_key = st.secrets[\"OPENAI_API_KEY\"]\n",
    "openai.base_url = st.secrets[\"OPENAI_BASE_URL\"]\n",
    "openai_model = st.secrets[\"OPENAI_MODEL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = \"å°Šæ•¬çš„ç”¨æˆ·ï¼Œä¸ºå“åº”å›½å®¶æ”¿ç­–ï¼Œæ”¯ä»˜å®ç°æ¨å‡ºé™æ¯ä¼˜æƒ ï¼Œè¯·ç™»å½•ä¸­å›½æ”¿åºœç½‘ç”Ÿæ´»å·éªŒè¯ä¿¡æ¯åï¼ŒæŒ‰ç…§æŒ‡å¼•æ“ä½œå³å¯äº«å—ä¼˜æƒ ã€‚\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniforge3\\envs\\DL\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "prediction = fraud_msg_cls.predict(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('è™šå‡ä¿¡ç”¨æœåŠ¡ç±»', 0.84811056),\n",
       " ('å†’å……ç”µå•†ç‰©æµå®¢æœç±»', 0.092546165),\n",
       " ('å†’å……å…¬æ£€æ³•åŠæ”¿åºœæœºå…³ç±»', 0.02540491),\n",
       " ('è™šå‡è´­ç‰©ã€æœåŠ¡ç±»', 0.013777527),\n",
       " ('æ— é£é™©', 0.0071956175),\n",
       " ('ç½‘é»‘æ¡ˆä»¶', 0.0050302274),\n",
       " ('å†’å……é¢†å¯¼ã€ç†Ÿäººç±»', 0.0036921047),\n",
       " ('å†’å……å†›è­¦è´­ç‰©ç±»è¯ˆéª—', 0.0018577599),\n",
       " ('è™šå‡ç½‘ç»œæŠ•èµ„ç†è´¢ç±»', 0.0013425661),\n",
       " ('ç½‘ç»œå©šæ‹ã€äº¤å‹ç±»', 0.0010425716)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "æˆ‘è¿™é‡Œæœ‰ä¸€æ¡ç–‘ä¼¼æ¬ºè¯ˆçš„ä¿¡æ¯ï¼Œä»¥ä¸‹æ˜¯ä¿¡æ¯å†…å®¹ï¼š\n",
    "{msg}\n",
    "\n",
    "ç”±æˆ‘è®­ç»ƒçš„æ¨¡å‹å¾—åˆ°ï¼Œè¯¥çŸ­ä¿¡å±äºçš„ç±»åˆ«çš„å„å¯èƒ½æ€§ä¸ºï¼š\n",
    "{prediction}\n",
    "\n",
    "è¯·ä½ æ ¹æ®æˆ‘è®­ç»ƒçš„æ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œé’ˆå¯¹æ¨¡å‹é¢„æµ‹çš„å¯èƒ½æ€§æœ€å¤§çš„çš„**ä¸€ç§ç±»åˆ«**ï¼Œç»™å‡ºé’ˆå¯¹æ”¶åˆ°çŸ­ä¿¡çš„é£é™©ç”¨æˆ·å»ºè®®ã€‚\n",
    "\n",
    "è¦æ±‚ï¼š\n",
    "1. ä¸è¦è¾“å‡ºæ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡å€¼\n",
    "2. å¯ä»¥é’ˆå¯¹çŸ­ä¿¡ä¸­çš„å†…å®¹çš„éƒ¨åˆ†ç‰¹å¾ï¼Œç»“åˆæ¨¡å‹é¢„æµ‹çš„ç±»åˆ«çš„å…¸å‹ç‰¹å¾ï¼Œç»™å‡ºé£é™©ç”¨æˆ·çš„å»ºè®®ã€‚\n",
    "3. å¦‚æœæ¨¡å‹é¢„æµ‹çš„ç±»åˆ«çš„å…¸å‹ç‰¹å¾ä¸è¶³ä»¥ç»™å‡ºå»ºè®®ï¼Œå¯ä»¥æ ¹æ®çŸ­ä¿¡çš„å†…å®¹ç»™å‡ºå»ºè®®ã€‚\n",
    "4. å¦‚æœæ¨¡å‹é¢„æµ‹ä¸ºæ— é£é™©ï¼Œå¯ä»¥æ­å–œç”¨æˆ·ï¼Œä½†ä¹Ÿå¯ä»¥ç»™å‡ºä¸€äº›å»ºè®®ã€‚\n",
    "5. åªéœ€è¦ç»™å‡ºçº¦ 200 å­—çš„å»ºè®®å³å¯ã€‚å»ºè®®æœ‰æ¡ç†åœ°åˆ—å‡ºã€‚\n",
    "6. é€‚é‡åŠ å…¥ emoji è¡¨æƒ…ï¼Œä½¿å¾—å»ºè®®æ›´åŠ ç”ŸåŠ¨æœ‰è¶£ã€‚\n",
    "\n",
    "å»ºè®®å†…å®¹ï¼š\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ ¹æ®æ¨¡å‹é¢„æµ‹ï¼Œè¿™æ¡çŸ­ä¿¡å±äº**è™šå‡ä¿¡ç”¨æœåŠ¡ç±»**è¯ˆéª—ã€‚ä»¥ä¸‹æ˜¯ç»™æ‚¨çš„å»ºè®®ï¼š\n",
      "\n",
      "1. **æ ¸å®å®˜æ–¹æ¸ é“** ğŸš¨  \n",
      "   æ”¯ä»˜å®ä¸ä¼šé€šè¿‡çŸ­ä¿¡è¦æ±‚ç”¨æˆ·ç™»å½•å…¶ä»–ç½‘ç«™éªŒè¯ä¿¡æ¯ã€‚ä»»ä½•æ¶‰åŠâ€œå›½å®¶æ”¿ç­–â€â€œé™æ¯ä¼˜æƒ â€ç­‰è¯æœ¯ï¼ŒåŠ¡å¿…é€šè¿‡æ”¯ä»˜å®å®˜æ–¹APPæˆ–å®¢æœç”µè¯ï¼ˆ95188ï¼‰æ ¸å®ã€‚\n",
      "\n",
      "2. **è­¦æƒ•é™Œç”Ÿé“¾æ¥** ğŸ”—  \n",
      "   çŸ­ä¿¡ä¸­çš„â€œä¸­å›½æ”¿åºœç½‘ç”Ÿæ´»å·â€å¯èƒ½æ˜¯ä¼ªé€ é“¾æ¥ï¼Œåˆ‡å‹¿ç›´æ¥ç‚¹å‡»ã€‚æ”¿åºœæœºæ„ä¸ä¼šé€šè¿‡ç§äººè´¦å·æ¨é€é‡‘èæ“ä½œæŒ‡å¼•ã€‚\n",
      "\n",
      "3. **ä¿æŠ¤ä¸ªäººä¿¡æ¯** ğŸ”  \n",
      "   åˆ‡å‹¿åœ¨é™Œç”Ÿé¡µé¢è¾“å…¥é“¶è¡Œå¡å·ã€å¯†ç æˆ–çŸ­ä¿¡éªŒè¯ç ï¼è™šå‡ä¿¡ç”¨æœåŠ¡å¸¸ä»¥â€œä¼˜æƒ â€ä¸ºè¯±é¥µï¼Œå®ä¸ºçªƒå–è´¦æˆ·ä¿¡æ¯ã€‚\n",
      "\n",
      "4. **ä¸¾æŠ¥å¯ç–‘å†…å®¹** ğŸ“¢  \n",
      "   å¯å°†çŸ­ä¿¡è½¬å‘è‡³æ”¯ä»˜å®ä¸¾æŠ¥å…¥å£ï¼ˆAPPå†…â€œæˆ‘çš„-å¸®åŠ©ä¸åé¦ˆâ€ï¼‰æˆ–åè¯ˆä¸“çº¿96110ï¼Œå¸®åŠ©ä»–äººé¿å…å—éª—ã€‚\n",
      "\n",
      "è®°ä½ï¼šå¤©ä¸‹æ²¡æœ‰å…è´¹çš„åˆé¤ï¼Œä¼˜æƒ æ´»åŠ¨è¯·è®¤å‡†å®˜æ–¹å…¬å‘Šï¼ ğŸ’¡\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(msg=msg, prediction=prediction)\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=openai_model,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"The following is a message that I received from a user and I need your help to respond to it.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "  ],\n",
    "  max_tokens=1024,\n",
    "  temperature=1.0,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
